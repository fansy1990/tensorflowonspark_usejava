# spark properties
spark.yarn.jar=hdfs://s0:8020/user/root/spark-assembly-1.5.1-hadoop2.6.0.jar
spark.yarn.scheduler.heartbeat.interval-ms=1000
spark.yarn.appMasterEnv.SPARK_YARN_USER_ENV=PYSPARK_PYTHON=Python/bin/python
spark.yarn.appMasterEnv.PYSPARK_PYTHON=Python/bin/python

spark.executor.instances=2

spark.files=hdfs://s0:8020/user/root/yarn-site.xml,hdfs://s0:8020/user/root/hive-site.xml,hdfs://s0:8020/user/root/core-site.xml,hdfs://s0:8020/user/root/hdfs-site.xml,hdfs://s0:8020/user/root/mapred-site.xml,hdfs://s0:8020/user/root/spark-defaults.conf


spark.yarn.appMasterEnv.LIB_HDFS=/usr/local/hadoop-2.7.4/lib/native
spark.yarn.appMasterEnv.LIB_JVM=/usr/local/jdk1.8.0_51/jre/lib/amd64/server
spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/usr/local/jdk1.8.0_51/jre/lib/amd64/server:/usr/local/hadoop-2.7.4/lib/native

spark.dynamicAllocation.enabled=false
spark.yarn.maxAppAttempts=1
spark.yarn.executor.memoryOverhead=348
spark.executorEnv.LD_LIBRARY_PATH=/usr/local/jdk1.8.0_51/jre/lib/amd64/server:/usr/local/hadoop-2.7.4/lib/native