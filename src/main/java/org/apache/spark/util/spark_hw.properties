# spark properties
spark.yarn.jar=hdfs://hacluster/user/root/tipdm/spark-assembly-1.5.1-hadoop2.7.2.jar
spark.yarn.scheduler.heartbeat.interval-ms=1000
spark.yarn.appMasterEnv.SPARK_YARN_USER_ENV=PYSPARK_PYTHON=Python/bin/python
spark.yarn.appMasterEnv.PYSPARK_PYTHON=Python/bin/python

spark.executor.instances=4

spark.files=hdfs://hacluster/user/root/tipdm/yarn-site.xml,hdfs://hacluster/user/root/tipdm/hive-site.xml,hdfs://hacluster/user/root/tipdm/core-site.xml,hdfs://hacluster/user/root/tipdm/hdfs-site.xml,hdfs://hacluster/user/root/tipdm/mapred-site.xml,hdfs://hacluster/user/root/tipdm/spark-defaults.conf



spark.yarn.appMasterEnv.LIB_HDFS=/opt/hadoopclient_new/HDFS/hadoop/lib/native
spark.yarn.appMasterEnv.LIB_JVM=/opt/hadoopclient_new/JDK/jdk1.8.0_131/jre/lib/amd64/server
spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/opt/hadoopclient_new/JDK/jdk1.8.0_131/jre/lib/amd64/server:/opt/hadoopclient_new/HDFS/hadoop/lib/native
#/opt/hadoopclient_new/JDK/jdk1.8.0_131/jre/lib/amd64/server:/opt/hadoopclient_new/HDFS/hadoop/lib/native
#spark.yarn.appMasterEnv.LIB_HDFS=/usr/local/hadoop-2.7.4/lib/native
#spark.yarn.appMasterEnv.LIB_JVM=/usr/local/jdk1.8.0_51/jre/lib/amd64/server
#spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/usr/local/jdk1.8.0_51/jre/lib/amd64/server:/usr/local/hadoop-2.7.4/lib/native

spark.dynamicAllocation.enabled=false
spark.yarn.maxAppAttempts=1
spark.yarn.executor.memoryOverhead=1024
spark.executorEnv.LD_LIBRARY_PATH=/opt/hadoopclient_new/JDK/jdk1.8.0_131/jre/lib/amd64/server:/opt/hadoopclient_new/HDFS/hadoop/lib/native
